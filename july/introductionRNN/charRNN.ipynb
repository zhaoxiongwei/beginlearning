{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于字符模型的，文本生产模型\n",
    "https://github.com/karpathy/char-rnn 项目是一个开源的文本自动生成器，支持生成各种类型文本。\n",
    "\n",
    "该项目的主要思路就是采用多层的LSTM训练RNN。\n",
    "\n",
    "## 字符模型\n",
    "\n",
    "首先网络采用字符模型，即输入的是字符向量，输出是字符向量＋softmax。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocb = torch.load('vocab.t7')\n",
    "print(vocb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络模型\n",
    "\n",
    "### LSTM 单元实现\n",
    "\n",
    "<img src=\"./images/LSTM.jpg\">\n",
    "\n",
    "char-rnn 采用的是多层LSTM（当然也支持另外一种常用的单元类型GRU），首先我们来看看LSTM单元的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "local LSTM = {}\n",
    "function LSTM.lstm(input_size, rnn_size, n, dropout)   \n",
    "  dropout = dropout or 0 \n",
    "\n",
    "  -- n          指的是级联的 LSTM 层数，每个LSTM需要保存h_prev以及c_prev\n",
    "  -- rnn_size   指隐变量的维度\n",
    "  -- input_size 字符向量的维度\n",
    "\n",
    "  -- there will be 2*n+1 inputs\n",
    "  local inputs = {}\n",
    "  table.insert(inputs, nn.Identity()()) -- x\n",
    "  for L = 1,n do\n",
    "    table.insert(inputs, nn.Identity()()) -- prev_c[L]\n",
    "    table.insert(inputs, nn.Identity()()) -- prev_h[L]\n",
    "  end\n",
    "\n",
    "  local x, input_size_L\n",
    "  local outputs = {}\n",
    "  for L = 1,n do\n",
    "    -- c,h from previos timesteps\n",
    "    local prev_h = inputs[L*2+1]\n",
    "    local prev_c = inputs[L*2]\n",
    "    -- the input to this layer\n",
    "    if L == 1 then \n",
    "      x = OneHot(input_size)(inputs[1])\n",
    "      input_size_L = input_size\n",
    "    else \n",
    "      x = outputs[(L-1)*2]                 -- 上一层的当前时间的h作为当前层的输出\n",
    "      if dropout > 0 then x = nn.Dropout(dropout)(x) end -- apply dropout, if any\n",
    "      input_size_L = rnn_size\n",
    "    end\n",
    "    -- evaluate the input sums at once for efficiency\n",
    "    local i2h = nn.Linear(input_size_L, 4 * rnn_size)(x)    -- 4组系数\n",
    "    local h2h = nn.Linear(rnn_size, 4 * rnn_size)(prev_h)   \n",
    "    local all_input_sums = nn.CAddTable()({i2h, h2h})\n",
    "\n",
    "    local reshaped = nn.Reshape(4, rnn_size)(all_input_sums)\n",
    "    local n1, n2, n3, n4 = nn.SplitTable(2)(reshaped):split(4)\n",
    "    -- decode the gates\n",
    "    local in_gate = nn.Sigmoid()(n1)\n",
    "    local forget_gate = nn.Sigmoid()(n2)\n",
    "    local out_gate = nn.Sigmoid()(n3)\n",
    "    -- decode the write inputs\n",
    "    local in_transform = nn.Tanh()(n4)\n",
    "    -- perform the LSTM update\n",
    "    local next_c           = nn.CAddTable()({               -- 对应的是公式中的z\n",
    "        nn.CMulTable()({forget_gate, prev_c}),\n",
    "        nn.CMulTable()({in_gate,     in_transform})\n",
    "      })\n",
    "    -- gated cells form the output\n",
    "    local next_h = nn.CMulTable()({out_gate, nn.Tanh()(next_c)})\n",
    "    \n",
    "    table.insert(outputs, next_c)\n",
    "    table.insert(outputs, next_h)                            \n",
    "  end\n",
    "\n",
    "  -- set up the decoder\n",
    "  local top_h = outputs[#outputs]                           -- 最后一个h输出＋softmax\n",
    "  if dropout > 0 then top_h = nn.Dropout(dropout)(top_h) end\n",
    "  local proj = nn.Linear(rnn_size, input_size)(top_h)       -- 把RNN -> 字符向量\n",
    "  local logsoft = nn.LogSoftMax()(proj)\n",
    "  table.insert(outputs, logsoft)\n",
    "\n",
    "  return nn.gModule(inputs, outputs)\n",
    "end\n",
    "\n",
    "return LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/multipleLSTM.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络训练\n",
    "\n",
    "网络训练和之前介绍的BPTT算法类似，需要注意的是通过矩阵化的并行计算以及Clip操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- do fwd/bwd and return loss, grad_params\n",
    "local init_state_global = clone_list(init_state)\n",
    "function feval(x)\n",
    "    if x ~= params then\n",
    "        params:copy(x)\n",
    "    end\n",
    "    grad_params:zero()\n",
    "\n",
    "    ------------------ get minibatch -------------------\n",
    "    local x, y = loader:next_batch(1)\n",
    "    if opt.gpuid >= 0 and opt.opencl == 0 then -- ship the input arrays to GPU\n",
    "        -- have to convert to float because integers can't be cuda()'d\n",
    "        x = x:float():cuda()\n",
    "        y = y:float():cuda()\n",
    "    end\n",
    "    if opt.gpuid >= 0 and opt.opencl == 1 then -- ship the input arrays to GPU\n",
    "        x = x:cl()\n",
    "        y = y:cl()\n",
    "    end\n",
    "    ------------------- forward pass -------------------\n",
    "    local rnn_state = {[0] = init_state_global}\n",
    "    local predictions = {}           -- softmax outputs\n",
    "    local loss = 0\n",
    "    for t=1,opt.seq_length do\n",
    "        clones.rnn[t]:training() -- make sure we are in correct mode (this is cheap, sets flag)\n",
    "        local lst = clones.rnn[t]:forward{x[{{}, t}], unpack(rnn_state[t-1])}\n",
    "        rnn_state[t] = {}\n",
    "        for i=1,#init_state do table.insert(rnn_state[t], lst[i]) end -- extract the state, without output\n",
    "        predictions[t] = lst[#lst] -- last element is the prediction\n",
    "        loss = loss + clones.criterion[t]:forward(predictions[t], y[{{}, t}])\n",
    "    end\n",
    "    loss = loss / opt.seq_length\n",
    "    ------------------ backward pass -------------------\n",
    "    -- initialize gradient at time t to be zeros (there's no influence from future)\n",
    "    local drnn_state = {[opt.seq_length] = clone_list(init_state, true)} -- true also zeros the clones\n",
    "    for t=opt.seq_length,1,-1 do\n",
    "        -- backprop through loss, and softmax/linear\n",
    "        local doutput_t = clones.criterion[t]:backward(predictions[t], y[{{}, t}])\n",
    "        table.insert(drnn_state[t], doutput_t)\n",
    "        local dlst = clones.rnn[t]:backward({x[{{}, t}], unpack(rnn_state[t-1])}, drnn_state[t])\n",
    "        drnn_state[t-1] = {}\n",
    "        for k,v in pairs(dlst) do\n",
    "            if k > 1 then -- k == 1 is gradient on x, which we dont need\n",
    "                -- note we do k-1 because first item is dembeddings, and then follow the \n",
    "                -- derivatives of the state, starting at index 2. I know...\n",
    "                drnn_state[t-1][k-1] = v\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    ------------------------ misc ----------------------\n",
    "    -- transfer final state to initial state (BPTT)\n",
    "    init_state_global = rnn_state[#rnn_state] -- NOTE: I don't think this needs to be a clone, right?\n",
    "    -- clip gradient element-wise\n",
    "    grad_params:clamp(-opt.grad_clip, opt.grad_clip)\n",
    "    return loss, grad_params\n",
    "end\n",
    "\n",
    "-- start optimization here\n",
    "train_losses = {}\n",
    "val_losses = {}\n",
    "local optim_state = {learningRate = opt.learning_rate, alpha = opt.decay_rate}\n",
    "local iterations = opt.max_epochs * loader.ntrain\n",
    "local iterations_per_epoch = loader.ntrain\n",
    "local loss0 = nil\n",
    "for i = 1, iterations do\n",
    "    local epoch = i / loader.ntrain\n",
    "\n",
    "    local timer = torch.Timer()\n",
    "    local _, loss = optim.rmsprop(feval, params, optim_state)\n",
    "    local time = timer:time().real\n",
    "\n",
    "    local train_loss = loss[1] -- the loss is inside a list, pop it\n",
    "    train_losses[i] = train_loss\n",
    "\n",
    "    -- exponential learning rate decay\n",
    "    if i % loader.ntrain == 0 and opt.learning_rate_decay < 1 then\n",
    "        if epoch >= opt.learning_rate_decay_after then\n",
    "            local decay_factor = opt.learning_rate_decay\n",
    "            optim_state.learningRate = optim_state.learningRate * decay_factor -- decay it\n",
    "            print('decayed learning rate by a factor ' .. decay_factor .. ' to ' .. optim_state.learningRate)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    -- handle early stopping if things are going really bad\n",
    "    if loss[1] ~= loss[1] then\n",
    "        print('loss is NaN.  This usually indicates a bug.  Please check the issues page for existing issues, or create a new issue, if none exist.  Ideally, please state: your operating system, 32-bit/64-bit, your blas version, cpu/cuda/cl?')\n",
    "        break -- halt\n",
    "    end\n",
    "    if loss0 == nil then loss0 = loss[1] end\n",
    "    if loss[1] > loss0 * 3 then\n",
    "        print('loss is exploding, aborting.')\n",
    "        break -- halt\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- start sampling/argmaxing\n",
    "for i=1, opt.length do\n",
    "\n",
    "    -- log probabilities from the previous timestep\n",
    "    if opt.sample == 0 then\n",
    "        -- use argmax\n",
    "        local _, prev_char_ = prediction:max(2)\n",
    "        prev_char = prev_char_:resize(1)\n",
    "    else\n",
    "        -- use sampling\n",
    "        prediction:div(opt.temperature) -- scale by temperature\n",
    "        local probs = torch.exp(prediction):squeeze()\n",
    "        probs:div(torch.sum(probs)) -- renormalize so probs sum to one\n",
    "        prev_char = torch.multinomial(probs:float(), 1):resize(1):float()\n",
    "    end\n",
    "\n",
    "    -- forward the rnn for next character\n",
    "    local lst = protos.rnn:forward{prev_char, unpack(current_state)}\n",
    "    current_state = {}\n",
    "    for i=1,state_size do table.insert(current_state, lst[i]) end\n",
    "    prediction = lst[#lst] -- last element holds the log probabilities\n",
    "\n",
    "    io.write(ivocab[prev_char[1]])\n",
    "end\n",
    "io.write('\\n') io.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
