{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Torch来构建RNN网络\n",
    "\n",
    "在Torch中可以采用nngraph组件来构建RNN，通过nngraph可以构造复杂结构的多输入、多输出的有向网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require('nngraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造多输入到单一输出\n",
    "\n",
    "这里构造一个两个向量输入，一个向量输出的简单网络，如下所示：\n",
    "\n",
    "<img src=\"./images/nto1.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- 构造神经网络每个独立的组件\n",
    "linearLayer1 = nn.Linear(3,2)\n",
    "linearLayer2 = nn.Linear(2,2)\n",
    "addLayer = nn.CAddTable()\n",
    "tanhLayer = nn.Tanh()\n",
    "linearLayer3 = nn.Linear(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- 构造多输入到单一输出的网络\n",
    "local inNode1 = linearLayer1()    -- 空括号表示，输入由runtime决定\n",
    "local inNode2 = linearLayer2()\n",
    "local addNode = addLayer({inNode1,inNode2})  -- ()表示输入Node\n",
    "local tanhNode = tanhLayer(addNode)\n",
    "local outNode = linearLayer3(tanhNode)\n",
    "\n",
    "model = nn.gModule({inNode1, inNode2}, {outNode})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = torch.Tensor({0.1, 1.5, -1.0})\n",
    "x2 = torch.Tensor({-1, 0})\n",
    "local y = model:forward({x1,x2})\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- 手动检验一下\n",
    "local l1 = linearLayer1:forward(x1)\n",
    "local l2 = linearLayer2:forward(x2)\n",
    "local add = addLayer:forward({l1,l2})\n",
    "local yp = tanhLayer:forward(add)\n",
    "local y = linearLayer3:forward(yp)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  多输入到多输出\n",
    "和之前的例子一样，只是增加一个输出，构造的网路如下所示：\n",
    "\n",
    "<img src=\"./images/nton.jpg\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- 构造多输入到多输出的网络\n",
    "local inNode1 = linearLayer1()    -- 空括号表示，输入由runtime决定\n",
    "local inNode2 = linearLayer2()\n",
    "local addNode = addLayer({inNode1,inNode2})  -- ()表示输入Node\n",
    "local tanhNode = tanhLayer(addNode)\n",
    "local outNode = linearLayer3(tanhNode)\n",
    "\n",
    "model = nn.gModule({inNode1, inNode2}, {outNode, addNode})    -- 输出增加一项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = model:forward({x1,x2})\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y[1], y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  构造递归结构\n",
    "\n",
    "由于我们得到了中间变量的输出，因此可以把中间变量给输出到下一次的输入中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local h0 = torch.rand(2)\n",
    "local x_t0 = torch.rand(3)\n",
    "local out_t1 = model:forward({x_t0, h0})\n",
    "\n",
    "print(out_t1[1])\n",
    "\n",
    "local x_t1 = torch.rand(3)\n",
    "local h1 = out_t1[2]                -- 得到h1项\n",
    "local out_t2 = model:forward({x_t1, h1}) \n",
    "\n",
    "print(out_t2[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过共享权重的克隆网络，实现BPTT算法\n",
    "\n",
    "为了能够通过nn模块提供的BP算法，实现BP through time算法，一般的做法是通过共享W矩阵的多个\"克隆\"网络，通过多次的BP来进行计算。\n",
    "\n",
    "\n",
    "### 1. 首先构造一个共享W（包括b）的神经网络组\n",
    "\n",
    "按应用的序列长度T来定义，神经网络组\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function createRNNCell()\n",
    "    local xInput = nn.Identity()()\n",
    "    local prev_h = nn.Identity()()\n",
    "    \n",
    "    local x2h = nn.Linear(2, 4)(xInput)\n",
    "    local h2h = nn.Linear(4, 4)(prev_h)\n",
    "    \n",
    "    local new_h = nn.CAddTable()({x2h,h2h})\n",
    "    local yout = nn.Sigmoid()(nn.Linear(4,1)(new_h))\n",
    "    \n",
    "    return nn.gModule({xInput, prev_h}, {yout, new_h})\n",
    "end\n",
    "\n",
    "-- 创建RNN组\n",
    "sequenceLength = 5\n",
    "sequenceRNN = {}\n",
    "for i=1,sequenceLength do\n",
    "  sequenceRNN[i] = createRNNCell()  \n",
    "end\n",
    "\n",
    "-- 共享参数\n",
    "local sharedPar,sharedGrad = sequenceRNN[1]:parameters()\n",
    "for i=2,sequenceLength do\n",
    "  local cloneParams, cloneGradParams = sequenceRNN[i]:parameters()\n",
    "  for j=1,#sharedPar do\n",
    "      cloneParams[j]:set(sharedPar[j])\n",
    "      cloneGradParams[j]:set(sharedGrad[j])\n",
    "  end\n",
    "end\n",
    "collectgarbage()\n",
    "\n",
    "--[[\n",
    "local temp,_ = sequenceRNN[2]:parameters()\n",
    "print( sharedPar[1] )\n",
    "\n",
    "sharedPar[1][{1,2}] = -1\n",
    "\n",
    "print( temp[1])\n",
    "--]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 前向计算\n",
    "\n",
    "准备输入和输出样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H0 = torch.rand(4)     -- h0 初始化\n",
    "Xs = {}                -- 输入序列   \n",
    "for i=1,#sequenceRNN do Xs[i] = torch.rand(2) end\n",
    "Ys = torch.Tensor({1.0});  -- 目标值\n",
    "criterion = nn.MSECriterion()  --构造一个简单二次回归\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forwardRNN = function()\n",
    "  local loss = 0.0\n",
    "  local prev_h = H0\n",
    "\n",
    "  for i=1,#sequenceRNN do\n",
    "    sequenceRNN[i]:evaluate()\n",
    "    local outs = sequenceRNN[i]:forward({Xs[i], prev_h})\n",
    "    loss = loss +  criterion:forward(outs[1], YP)   \n",
    "    prev_h = outs[2]\n",
    "  end\n",
    "    \n",
    "  return loss\n",
    "end\n",
    "\n",
    "print(  forwardRNN() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.通过逐层BP计算实现BPTT算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BackPropThroughTime = function()\n",
    "  local loss = 0.0\n",
    "  local prev_h = h0\n",
    "\n",
    "    \n",
    "  -- forward\n",
    "  hs = {}\n",
    "  outs = {}\n",
    "  for i=1,#sequenceRNN do\n",
    "    sequenceRNN[i]:training()\n",
    "    local ys = sequenceRNN[i]:forward({Xs[i], prev_h})\n",
    "    hs[i] = ys[2]\n",
    "    outs[i] = ys[1]\n",
    "    prev_h = ys[2]     \n",
    "  end\n",
    "  \n",
    "    \n",
    "  --backward\n",
    "  -- 首先将grad置位为0  \n",
    "  local _,grad = sequenceRNN[1]:parameters()\n",
    "  for i=1,#grad do\n",
    "    grad[1]:zero()\n",
    "  end\n",
    "  \n",
    "  hs[0] = h0\n",
    "  dh = torch.zeros(4)\n",
    "  for i=#sequenceRNN,1,-1 do\n",
    "    local dy = criterion:backward(outs[i], Ys)\n",
    "    local dout = sequenceRNN[i]:backward({Xs[i], hs[i-1]}, {dy, dh})\n",
    "    dh = dout[2]\n",
    "  end\n",
    "  \n",
    "  return grad\n",
    "end\n",
    "\n",
    "bpttGrad = BackPropThroughTime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.用Number Checker进行校验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local pra,_ = sequenceRNN[1]:parameters()\n",
    "\n",
    "pra[1][2][2] = pra[1][2][2] + 0.0001\n",
    "local lossRight = forwardRNN()\n",
    "pra[1][2][2] = pra[1][2][2] - 0.0002\n",
    "local lossLeft = forwardRNN()\n",
    "\n",
    "local dw11 = (lossRight - lossLeft) / (2*0.0001)\n",
    "\n",
    "print (lossRight, lossLeft, dw11)\n",
    "\n",
    "print ( bpttGrad[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
