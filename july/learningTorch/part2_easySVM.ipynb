{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Troch框架实现简易版本的 SVM\n",
    "\n",
    "\n",
    "Troch是一个通用的机器学习框架，可以使用 lua 语言（支持绑定C语言实现）进行各种机器学习开发，类似于使用Matlab。\n",
    "\n",
    "这里给出一个使用torch开发的简易版本的SVM。\n",
    "\n",
    "\n",
    "### 0. 装载训练数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showData = require('./showData')\n",
    "trainData = require('./trainData')\n",
    "showData(trainData.x, trainData.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 初始化SVM，训练SVM内部参数\n",
    "\n",
    "* 线性核 vs 高斯核函数\n",
    "\n",
    "* 关键参数C的选择\n",
    "\n",
    "* 通过SMO算法寻找一组$\\alpha$，满足KTT条件\n",
    "\n",
    "#### 带核函数的软间隔 SVM 问题描述：\n",
    "\n",
    "求解目标为：\n",
    "\n",
    "$$\n",
    "\\newcommand{\\argmin}{\\mathop{\\rm arg~min}\\limits}\n",
    "\\argmin_{\\alpha_n} ( \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N \\alpha_n\\alpha_my_ny_mK(x_n,x_m) - \\sum_{n=1}^N \\alpha_n )\n",
    "$$\n",
    "\n",
    "同时满足两个约束条件\n",
    "\n",
    "$$\n",
    "C\\geq\\alpha_n\\geq0 \\\\\n",
    "\\sum_{n=1}^N \\alpha_ny_n = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "在求解到$\\alpha_n$值之后，不再直接求解w的值，而是直接带入新的x进行计算，不需要（某些情况也做不到）保存W\n",
    "\n",
    "$$\n",
    "h(x') = \\sum_{n=1}^N \\alpha_ny_nk(x_n,x') + b\n",
    "$$\n",
    "\n",
    "其中b的计算也是通过核函数完成，选择一个自由项$\\alpha_j$,\n",
    "\n",
    "$$\n",
    "b = y_j - \\sum_{n=1}^N \\alpha_ny_nk(x_n,x_j)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "easySVM = require('./easySVM')\n",
    "svm = easySVM({kernel='linear', C=1.0})  --注意重要的三个参数，kernel, C, sigma\n",
    "\n",
    "local logValue = svm:train( trainData.x, trainData.y)\n",
    "\n",
    "local logSeq = {}\n",
    "\n",
    "for i=1,#logValue do \n",
    "  logSeq[i] = i \n",
    "end \n",
    "\n",
    "--可视化最优化目标\n",
    "plot = Plot():line(logSeq, logValue,'red','example'):legend(true):title('训练过程'):draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 测试SVM分类效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local marginLineX = {}\n",
    "local marginLineY = {}\n",
    "local singleSample = torch.Tensor(2)\n",
    "\n",
    "for x1=-1.5, 1.5, 0.03 do\n",
    "    for x2=-1.5, 1.5, 0.03 do\n",
    "        singleSample[1] = x1\n",
    "        singleSample[2] = x2\n",
    "        local y = svm:pred(singleSample)\n",
    "        if ( math.abs(y) < 0.05 ) then\n",
    "            marginLineX[#marginLineX+1] = x1\n",
    "            marginLineY[#marginLineY+1] = x2\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "plot = showData(trainData.x, trainData.y)\n",
    "plot:line(marginLineX,marginLineY,'black', 'yolo'):redraw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 源代码解读\n",
    "\n",
    "实现核心是SMO算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require('torch')\n",
    "\n",
    "easySVM = function (options)\n",
    "  svm = {}\n",
    "  options = options or {}\n",
    "\n",
    "  function svm:_init()\n",
    "    -- 设置默认参数\n",
    "    self.C = options.C or 1.0\n",
    "    self.tol = options.tol or 1e-5\n",
    "    self.sigma = options.sigma or 0.5\n",
    "\n",
    "    local kernel = options.kernel or 'linear'\n",
    "    if ( kernel == 'linear' ) then\n",
    "      self.kernel = self._linear\n",
    "    elseif (kernel == 'rbf' ) then\n",
    "      self.kernel = self._rbf\n",
    "    else\n",
    "      self.kernel = self._linear\n",
    "    end\n",
    "  end\n",
    "\n",
    "  -- 训练函数\n",
    "  function svm:train(samples, labels, maxIterate)\n",
    "    local i, j\n",
    "\n",
    "    local targetHistory = {}\n",
    "\n",
    "    -- 向量初始化\n",
    "    self.x = {}\n",
    "    self.y = {}\n",
    "    self.alphas = {}\n",
    "    self.b = 0.0\n",
    "\n",
    "    for i = 1, #samples do\n",
    "      self.alphas[i] = 0.0\n",
    "      self.x[i] = samples[i]\n",
    "      if ( labels[i][1] == 1) then\n",
    "        self.y[i] = 1\n",
    "      else\n",
    "        self.y[i] = -1\n",
    "      end\n",
    "    end\n",
    "\n",
    "    -- for SMO algorithm\n",
    "    local Ei, Ej\n",
    "    local ai, aj, ai_, aj_\n",
    "    local b1, b2\n",
    "    local L, H\n",
    "    local eta\n",
    "\n",
    "    maxIterate = maxIterate or 200\n",
    "    local iter = 0\n",
    "    local passes = 0\n",
    "    local alphaChaned = 0\n",
    "\n",
    "    -- SMO 算法实现\n",
    "    while (passes < 10 and iter < maxIterate ) do\n",
    "      alphaChaned = 0\n",
    "      for i = 1, #self.alphas do\n",
    "        -- 选择对应的alpha_1\n",
    "        Ei = (self:pred(self.x[i]) - self.y[i]) * self.y[i]\n",
    "        if (  (Ei < -1 * self.tol and self.alphas[i] < self.C)\n",
    "            or (Ei > self.tol and self.alphas[i] > 0) ) then\n",
    "\n",
    "          Ei = Ei * self.y[i]\n",
    "\n",
    "          j = i\n",
    "          while(j == i) do\n",
    "            j = math.floor( math.random() * #self.alphas + 1)\n",
    "          end\n",
    "          Ej = self:pred(self.x[j]) - self.y[j]\n",
    "\n",
    "          ai = self.alphas[i]\n",
    "          aj = self.alphas[j]\n",
    "\n",
    "          if ( self.y[i] == self.y[j] ) then\n",
    "            L = math.max(0, ai + aj - self.C)\n",
    "            H = math.min(self.C , ai + aj)\n",
    "          else\n",
    "            L = math.max(0, aj - ai)\n",
    "            H = math.min(self.C, self.C + aj - ai)\n",
    "          end\n",
    "\n",
    "          eta = 2 * self:kernel(self.x[i], self.x[j]) - self:kernel(self.x[i], self.x[i]) - self:kernel(self.x[j], self.x[j])\n",
    "          aj_ = aj - self.y[j] * (Ei - Ej) / eta\n",
    "          if ( aj_  > H ) then\n",
    "            aj_ = H\n",
    "          end\n",
    "          if ( aj_ < L ) then\n",
    "            aj_ = L\n",
    "          end\n",
    "\n",
    "\n",
    "          -- 更新 alpha_i alpha_j b\n",
    "          if ( math.abs(L-H) > 1e-4 and eta < 0 and math.abs(aj - aj_) > 1e-4) then\n",
    "\n",
    "            self.alphas[j] = aj_\n",
    "            ai_ = ai + self.y[i] * self.y[j] * ( aj - aj_)\n",
    "            self.alphas[i] = ai_\n",
    "\n",
    "            --update b\n",
    "            b1 = self.b - Ei - self.y[i]*(ai_ - ai)*self:kernel(self.x[i], self.x[i])\n",
    "                 - self.y[j]*(aj_ - aj)*self:kernel(self.x[i], self.x[j])\n",
    "\n",
    "            b2 = self.b - Ej - self.y[j]*(aj_ - aj)*self:kernel(self.x[i], self.x[j])\n",
    "                 - self.y[j]*(aj_ - aj)*self:kernel(self.x[j], self.x[j])\n",
    "\n",
    "            self.b = (b1+b2)/2\n",
    "            if ( ai_ > 0 and ai_ < self.C) then\n",
    "              self.b = b1\n",
    "            end\n",
    "            if ( aj_ > 0 and aj_ < self.C) then\n",
    "              self.b = b2\n",
    "            end\n",
    "            alphaChaned = alphaChaned + 1\n",
    "\n",
    "          end  -- end of i and j is OK\n",
    "        end -- end of selected i\n",
    "     end -- end of all i\n",
    "\n",
    "     iter = iter + 1\n",
    "     if(alphaChaned == 0) then\n",
    "       passes = passes + 1\n",
    "     else\n",
    "       passes = 0;\n",
    "     end\n",
    "\n",
    "     local targetValue = self:minTarget()\n",
    "     targetHistory[#targetHistory+1] = targetValue\n",
    "\n",
    "   end -- end of iterator\n",
    "\n",
    "   print(\"SVM training is done, total iterator number:\" .. iter)\n",
    "\n",
    "   return targetHistory\n",
    "  end\n",
    "\n",
    "  -- 分类函数\n",
    "  function svm:pred(x)\n",
    "    local ret = 0.0;\n",
    "    for i=1, #self.alphas do\n",
    "      ret = ret + self.alphas[i] * self.y[i] * self:kernel(x, self.x[i])\n",
    "    end\n",
    "     ret = ret + self.b\n",
    "     return ret\n",
    "  end\n",
    "\n",
    "  function svm:minTarget()\n",
    "    local targetValue = 0.0\n",
    "    for i = 1, #self.alphas do\n",
    "      for j = 1, #self.alphas do\n",
    "        targetValue = targetValue + self.alphas[i]*self.alphas[j]*self.y[i]*self.y[j]*self:kernel(self.x[i], self.x[j])\n",
    "      end\n",
    "    end\n",
    "\n",
    "    for i = 1, #self.alphas do\n",
    "      targetValue = targetValue - self.alphas[i]\n",
    "    end\n",
    "\n",
    "    return targetValue\n",
    "  end\n",
    "\n",
    "  -- 内置核函数\n",
    "  function svm:_rbf(v1, v2)\n",
    "    local s = 0;\n",
    "    for i=1, v1:size()[1] do\n",
    "      s = s + (v1[i] - v2[i]) * (v1[i] - v2[i])\n",
    "    end\n",
    "    s = torch.exp( -1 * s / (2.0 * self.sigma * self.sigma ) )\n",
    "    return s\n",
    "  end\n",
    "\n",
    "  function svm:_linear(v1, v2)\n",
    "    local s = 0;\n",
    "    for i=1, v1:size()[1] do\n",
    "      s = s + v1[i] * v2[i]\n",
    "    end\n",
    "    return s\n",
    "  end\n",
    "\n",
    "\n",
    "  svm:_init()\n",
    "  return svm\n",
    "\n",
    "end\n",
    "\n",
    "return easySVM\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language": "lua",
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
